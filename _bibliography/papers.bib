---
---

@string{CVPR = {Proceedings of the {IEEE/CVF} Conference on Computer Vision and Pattern Recognition (<b>CVPR</b>)}}
@string{ICCV = {Proceedings of the {IEEE/CVF} Conference on International Conference on Computer Vision (<b>ICCV</b>)}}
@string{WACV = {Proceedings of the {IEEE/CVF} Winter Conference on Applications of Computer Vision (<b>WACV</b>)}}

@string{arXiv = {arXiv preprint,}}


@InProceedings{Kim_2023_CVPR,
    author    = {Sanghyun Kim and Deunsol Jung and Minsu Cho},
    title     = {Locality-Aware Interaction for Zero-Shot Human-Object Interaction Detection},
    booktitle = CVPR,
    year      = {2025},
    selected = {true},
    abbr = {CVPR},
    preview = {coming.png}
}

@inproceedings{kim2024burst,
  title={Burst Image Super-Resolution with Base Frame Selection},
  author={Sanghyun Kim and Minjung Lee and Woohyeok Kim and Deunsol Jung and Jaesung Rim and Sunghyun Cho and Minsu Cho},
  booktitle={Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition Workshop on New Trends in Image Restroation and Enhancement (<b>NTIRE</b>)},
  pages={5940--5949},
  year={2024},
  selected = {true},
  abstract={Burst image super-resolution has been a topic of active research in recent years due to its ability to obtain a high-resolution image by using complementary information between multiple frames in the burst. In this work, we explore using burst shots with non-uniform exposures to confront real-world practical scenarios by introducing a new benchmark dataset, dubbed Non-uniformly Exposed Burst Image (NEBI), that includes the burst frames at varying exposure times to obtain a broader range of irradiance and motion characteristics within a scene. As burst shots with non-uniform exposures exhibit varying levels of degradation, fusing information of the burst shots into the first frame  as a base frame may not result in optimal image quality. To address this limitation, we propose a Frame Selection Network (FSN) for non-uniform scenarios. This network seamlessly integrates into existing super-resolution methods in a plug-and-play manner with low computational costs. The comparative analysis reveals the effectiveness of the nonuniform setting for the practical scenario and our FSN on synthetic-/real- NEBI datasets.},
  abbr = {CVPRW},
  arxiv= {2406.17869},
  website={https://postech-cvlab.github.io/Burst_FSN/},
  preview={FSN.PNG}
}


@InProceedings{Yoon_2024_WACV,
    author    = {Jeongbeen Yoon and Sanghyun Kim and Suha Kwak and Minsu Cho},
    title     = {Optical Flow Domain Adaptation via Target Style Transfer},
    booktitle = WACV,
    year      = {2024},
    pages     = {2111-2121},
    selected = {true},
    abstract={Optical flows play an integral role for a variety of motion-related tasks such as action recognition, object segmentation, and tracking in videos. While state-of-the-art optical flow methods heavily rely on learning, the learned optical flow methods significantly degrade when applied to different domains, and the training datasets are very limited due to the extreme cost of flow-level annotation. To tackle the issue, we introduce a domain adaptation technique for optical flow estimation. Our method extracts diverse style statistics of the target domain and use them in training to generate synthetic features from the source features, which contain the contents of the source but the style of the target. We also impose motion consistency between the synthetic target and the source and deploy adversarial learning at the flow prediction to encourage domain-invariant features. Experimental results show that the proposed method achieves substantial and consistent improvements in different domain adaptation scenarios on VKITTI 2, Sintel, and KITTI 2015 benchmarks.},
    abbr = {WACV},
    website={https://openaccess.thecvf.com/content/WACV2024/html/Yoon_Optical_Flow_Domain_Adaptation_via_Target_Style_Transfer_WACV_2024_paper.html},
    preview = {OFDA.PNG}
}

@InProceedings{Kim_2023_CVPR,
    author    = {Sanghyun Kim and Deunsol Jung and Minsu Cho},
    title     = {Relational Context Learning for Human-Object Interaction Detection},
    booktitle = CVPR,
    year      = {2023},
    pages     = {2925-2934},
    selected = {true},
    abstract={Recent state-of-the-art methods for HOI detection typically build on transformer architectures with two decoder branches, one for human-object pair detection and the other for interaction classification. Such disentangled transformers, however, may suffer from insufficient context exchange between the branches and lead to a lack of context information for relational reasoning, which is critical in discovering HOI instances. In this work, we propose the multiplex relation network (MUREN) that performs rich context exchange between three decoder branches using unary, pairwise, and ternary relations of human, object, and interaction tokens. The proposed method learns comprehensive relational contexts for discovering HOI instances, achieving state-of-the-art performance on two standard benchmarks for HOI detection, HICO-DET and V-COCO.
},
    website = {https://cvlab.postech.ac.kr/research/MUREN/},
    code = {https://github.com/OreoChocolate/MUREN},
    abbr = {CVPR},
    arxiv = {2304.04997},
    preview = {MUREN.PNG}
}

@InProceedings{Jung_2023_CVPR,
    author    = {Deunsol Jung and Sanghyun Kim and Won Hwa Kim and Minsu Cho},
    title     = {Devil's on the Edges: Selective Quad Attention for Scene Graph Generation},
    booktitle = CVPR,
    year      = {2023},
    pages     = {18664-18674},
    selected = {true},
    abstract={Scene graph generation aims to construct a semantic graph structure from an image such that its nodes and edges respectively represent objects and their relationships. One of the major challenges for the task lies in the presence of distracting objects and relationships in images; contextual reasoning is strongly distracted by irrelevant objects or backgrounds and, more importantly, a vast number of irrelevant candidate relations. To tackle the issue, we propose the Selective Quad Attention Network (SQUAT) that learns to select relevant object pairs and disambiguate them via diverse contextual interactions. SQUAT consists of two main components: edge selection and quad attention. The edge selection module selects relevant object pairs, i.e., edges in the scene graph, which helps contextual reasoning, and the quad attention module then updates the edge features using both edge-to-node and edge-to-edge cross-attentions to capture contextual information between objects and object pairs. Experiments demonstrate the strong performance and robustness of SQUAT, achieving the state of the art on the Visual Genome and Open Images v6 benchmarks},
    website = {https://cvlab.postech.ac.kr/research/SQUAT/},
    code = {https://github.com/hesedjds/SQUAT},
    abbr = {CVPR},
    arxiv = {2304.03495},
    preview = {SQUAT.PNG}
}


