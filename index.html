<!DOCTYPE html> <html lang="en"> <head> <meta http-equiv="Content-Type" content="text/html; charset=UTF-8"> <meta charset="utf-8"> <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"> <meta http-equiv="X-UA-Compatible" content="IE=edge"> <title> Sanghyun Kim </title> <meta name="author" content="Sanghyun Kim"> <meta name="description" content="A simple, whitespace theme for academics. Based on [*folio](https://github.com/bogoli/-folio) design. "> <meta name="keywords" content="jekyll, jekyll-theme, academic-website, portfolio-website"> <link rel="stylesheet" href="/assets/css/bootstrap.min.css?a4b3f509e79c54a512b890d73235ef04"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/css/mdb.min.css" integrity="sha256-jpjYvU3G3N6nrrBwXJoVEYI/0zw8htfFnhT9ljN3JJw=" crossorigin="anonymous"> <link defer rel="stylesheet" href="/assets/css/academicons.min.css?f0b7046b84e425c55f3463ac249818f5"> <link defer rel="stylesheet" href="/assets/css/scholar-icons.css?62b2ac103a88034e6882a5be5f3e2772"> <link defer rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700|Roboto+Slab:100,300,400,500,700|Material+Icons&amp;display=swap"> <link defer rel="stylesheet" href="/assets/css/jekyll-pygments-themes-github.css?591dab5a4e56573bf4ef7fd332894c99" media="" id="highlight_theme_light"> <link rel="shortcut icon" href="data:image/svg+xml,&lt;svg%20xmlns=%22http://www.w3.org/2000/svg%22%20viewBox=%220%200%20100%20100%22&gt;&lt;text%20y=%22.9em%22%20font-size=%2290%22&gt;%E2%9A%9B%EF%B8%8F&lt;/text&gt;&lt;/svg&gt;"> <link rel="stylesheet" href="/assets/css/main.css?d41d8cd98f00b204e9800998ecf8427e"> <link rel="canonical" href="https://oreochocolate.github.io/"> <script src="/assets/js/theme.js?9a0c749ec5240d9cda97bc72359a72c0"></script> <link defer rel="stylesheet" href="/assets/css/jekyll-pygments-themes-native.css?5847e5ed4a4568527aa6cfab446049ca" media="none" id="highlight_theme_dark"> <script>
    initTheme();
  </script> </head> <body class="fixed-top-nav "> <header> <nav id="navbar" class="navbar navbar-light navbar-expand-sm fixed-top" role="navigation"> <div class="container"> <button class="navbar-toggler collapsed ml-auto" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation"> <span class="sr-only">Toggle navigation</span> <span class="icon-bar top-bar"></span> <span class="icon-bar middle-bar"></span> <span class="icon-bar bottom-bar"></span> </button> <div class="collapse navbar-collapse text-right" id="navbarNav"> <ul class="navbar-nav ml-auto flex-nowrap"> <li class="nav-item active"> <a class="nav-link" href="/">About <span class="sr-only">(current)</span> </a> </li> <li class="nav-item "> <a class="nav-link" href="/publications/">Publications </a> </li> <li class="nav-item "> <a class="nav-link" href="/cv/">Curriculum Vitae </a> </li> <li class="toggle-container"> <button id="light-toggle" title="Change theme"> <i class="ti ti-sun-moon" id="light-toggle-system"></i> <i class="ti ti-moon-filled" id="light-toggle-dark"></i> <i class="ti ti-sun-filled" id="light-toggle-light"></i> </button> </li> </ul> </div> </div> </nav> <progress id="progress" value="0"> <div class="progress-container"> <span class="progress-bar"></span> </div> </progress> </header> <div class="container mt-5" role="main"> <div class="post"> <header class="post-header"> <h1 class="post-title"> <span class="font-weight-bold">Sanghyun</span> Kim </h1> <p class="desc">Ph.d candidate in the <a href="https://cvlab.postech.ac.kr/" rel="external nofollow noopener" target="_blank">Computer Vision Lab</a> at <a href="https://postech.ac.kr/" rel="external nofollow noopener" target="_blank">POSTECH</a></p> </header> <article> <div class="profile float-right"> <figure> <picture> <img src="/assets/img/shk.jpg?ed1ee9253ddc9df6398b90cda7f2d812" class="img-fluid z-depth-1 rounded-circle" width="100%" height="auto" alt="shk.jpg" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> <div class="clearfix"> <p>I am a Ph.D. candidate in the <a href="https://cse.postech.ac.kr/" rel="external nofollow noopener" target="_blank">Computer Science and Engineering (CSE)</a> department at POSTECH, working in the <a href="https://cvlab.postech.ac.kr/" rel="external nofollow noopener" target="_blank">Computer Vision Lab</a>, advised by Prof. <a href="https://cvlab.postech.ac.kr/~mcho/" rel="external nofollow noopener" target="_blank">Minsu Cho</a>.</p> <p>My research focuses on computer vision and deep learning, including but not limited to human behavior understanding, scene understanding, domain adaptation, computational imaging, and its applications. I am particularly interested in Human-Object Interaction (HOI) Detection, which goes beyond traditional object detection by not only localizing humans and objects but also recognizing their interactions. If you are interested in my research projects, please feel free to <a href="mailto:sanghyun.kim@postech.ac.kr">contact me</a>.</p> </div> <h2> <a href="/news/" style="color: inherit">News</a> </h2> <div class="news"> <div class="table-responsive" style="max-height: 60vw"> <table class="table table-sm table-borderless"> <tr> <th scope="row" style="width: 20%">Feb 27, 2025</th> <td> ðŸ“œ A paper on <i>zero-shot learning for Human-Object Interaction (HOI) Detection</i> has been accepted to CVPR 2025. </td> </tr> <tr> <th scope="row" style="width: 20%">Apr 12, 2024</th> <td> ðŸ“œ A paper on <i>burst image super-resolution</i> has been accepted to NTIRE at CVPRW 2024. </td> </tr> <tr> <th scope="row" style="width: 20%">Oct 25, 2023</th> <td> ðŸ“œ A paper on <i>domain adaptation for optical flow</i> has been accepted to WACV 2024. </td> </tr> <tr> <th scope="row" style="width: 20%">Feb 28, 2023</th> <td> ðŸ“œ Two papers on <i>Human-Object Interaction (HOI) Detection</i> and <i>Scene Graph Generation (SGG)</i> have been accepted to CVPR 2023. </td> </tr> </table> </div> </div> <div class="education"> <h2>Education</h2> <div class="table-responsive"> <table class="table table-sm table-borderless"> <tr> <th scope="row">Feb, 2019 - Present</th> <td> <a href="https://postech.ac.kr/eng/" rel="external nofollow noopener" target="_blank">Pohang University of Science and Technology (POSTECH)</a>, Pohang, South Korea <br> Integrated M.S./Ph.D. student in <a href="https://cse.postech.ac.kr/" rel="external nofollow noopener" target="_blank">Computer Science and Engineering</a> <br> GPA: 3.96/4.3 <br> Advisor: Prof. <a href="https://cvlab.postech.ac.kr/~mcho" rel="external nofollow noopener" target="_blank">Minsu Cho</a> </td> </tr> <tr> <th scope="row">Mar, 2014 - Feb, 2019</th> <td> <a href="https://ssu.ac.kr/" rel="external nofollow noopener" target="_blank">Soongsil University </a>, Seoul, South Korea <br> B.S. in <a href="https://cse.ssu.ac.kr/" rel="external nofollow noopener" target="_blank">Computer Science and Engineering</a> <br> GPA: 4.30/4.5 (Summa Cum Laude) <br> Advisor: Prof. <a href="http://ml.ssu.ac.kr/kbhwang/kbhwang.html" rel="external nofollow noopener" target="_blank">Kyubaek Hwang</a> </td> </tr> </table> </div> </div> <h2> <a href="/publications/" style="color: inherit">Publications</a> </h2> <div class="publications"> <ol class="bibliography"> <li> <div class="row"> <div class="col col-sm-4 abbr"> <abbr class="badge rounded w-100">CVPR</abbr> <figure> <picture> <img src="/assets/img/publication_preview/coming.png" class="preview z-depth-1 rounded" width="100%" height="auto" alt="coming.png" data-zoomable loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> <div id="Kim_2023_CVPR" class="col-sm-8"> <div class="title">Locality-Aware Interaction for Zero-Shot Human-Object Interaction Detection</div> <div class="author"> <em><i>Sanghyun Kim</i> </em>,Â Deunsol Jung,Â andÂ Minsu Cho </div> <div class="periodical"> <em>In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (<b>CVPR</b>)</em>, 2025 </div> <div class="periodical"> </div> <div class="links"> </div> </div> </div> </li> <li> <div class="row"> <div class="col col-sm-4 abbr"> <abbr class="badge rounded w-100">CVPRW</abbr> <figure> <picture> <img src="/assets/img/publication_preview/FSN.PNG" class="preview z-depth-1 rounded" width="100%" height="auto" alt="FSN.PNG" data-zoomable loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> <div id="kim2024burst" class="col-sm-8"> <div class="title">Burst Image Super-Resolution with Base Frame Selection</div> <div class="author"> <em><i>Sanghyun Kim</i> </em>,Â Minjung Lee,Â Woohyeok Kim,Â Deunsol Jung, and <span class="more-authors" title="click to view 3 more authors" onclick=" var element=$(this); element.attr('title', ''); var more_authors_text=element.text() == '3 more authors' ? 'Jaesung Rim, Sunghyun Cho, Minsu Cho' : '3 more authors'; var cursorPosition=0; var textAdder=setInterval(function(){ element.html(more_authors_text.substring(0, cursorPosition + 1)); if (++cursorPosition == more_authors_text.length){ clearInterval(textAdder); } }, '10'); ">3 more authors</span> </div> <div class="periodical"> <em>In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition Workshop on New Trends in Image Restroation and Enhancement (<b>NTIRE</b>)</em>, 2024 </div> <div class="periodical"> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a href="http://arxiv.org/abs/2406.17869" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">arXiv</a> <a href="https://postech-cvlab.github.io/Burst_FSN/" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">Website</a> </div> <div class="abstract hidden"> <p>Burst image super-resolution has been a topic of active research in recent years due to its ability to obtain a high-resolution image by using complementary information between multiple frames in the burst. In this work, we explore using burst shots with non-uniform exposures to confront real-world practical scenarios by introducing a new benchmark dataset, dubbed Non-uniformly Exposed Burst Image (NEBI), that includes the burst frames at varying exposure times to obtain a broader range of irradiance and motion characteristics within a scene. As burst shots with non-uniform exposures exhibit varying levels of degradation, fusing information of the burst shots into the first frame as a base frame may not result in optimal image quality. To address this limitation, we propose a Frame Selection Network (FSN) for non-uniform scenarios. This network seamlessly integrates into existing super-resolution methods in a plug-and-play manner with low computational costs. The comparative analysis reveals the effectiveness of the nonuniform setting for the practical scenario and our FSN on synthetic-/real- NEBI datasets.</p> </div> </div> </div> </li> <li> <div class="row"> <div class="col col-sm-4 abbr"> <abbr class="badge rounded w-100">WACV</abbr> <figure> <picture> <img src="/assets/img/publication_preview/OFDA.PNG" class="preview z-depth-1 rounded" width="100%" height="auto" alt="OFDA.PNG" data-zoomable loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> <div id="Yoon_2024_WACV" class="col-sm-8"> <div class="title">Optical Flow Domain Adaptation via Target Style Transfer</div> <div class="author"> Jeongbeen Yoon,Â <em><i>Sanghyun Kim</i> </em>,Â Suha Kwak,Â andÂ Minsu Cho </div> <div class="periodical"> <em>In Proceedings of the IEEE/CVF Winter Conference on Applications of Computer Vision (<b>WACV</b>)</em>, 2024 </div> <div class="periodical"> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a href="https://openaccess.thecvf.com/content/WACV2024/html/Yoon_Optical_Flow_Domain_Adaptation_via_Target_Style_Transfer_WACV_2024_paper.html" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">Website</a> </div> <div class="abstract hidden"> <p>Optical flows play an integral role for a variety of motion-related tasks such as action recognition, object segmentation, and tracking in videos. While state-of-the-art optical flow methods heavily rely on learning, the learned optical flow methods significantly degrade when applied to different domains, and the training datasets are very limited due to the extreme cost of flow-level annotation. To tackle the issue, we introduce a domain adaptation technique for optical flow estimation. Our method extracts diverse style statistics of the target domain and use them in training to generate synthetic features from the source features, which contain the contents of the source but the style of the target. We also impose motion consistency between the synthetic target and the source and deploy adversarial learning at the flow prediction to encourage domain-invariant features. Experimental results show that the proposed method achieves substantial and consistent improvements in different domain adaptation scenarios on VKITTI 2, Sintel, and KITTI 2015 benchmarks.</p> </div> </div> </div> </li> <li> <div class="row"> <div class="col col-sm-4 abbr"> <abbr class="badge rounded w-100">CVPR</abbr> <figure> <picture> <img src="/assets/img/publication_preview/MUREN.PNG" class="preview z-depth-1 rounded" width="100%" height="auto" alt="MUREN.PNG" data-zoomable loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> <div id="Kim_2023_CVPS" class="col-sm-8"> <div class="title">Relational Context Learning for Human-Object Interaction Detection</div> <div class="author"> <em><i>Sanghyun Kim</i> </em>,Â Deunsol Jung,Â andÂ Minsu Cho </div> <div class="periodical"> <em>In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (<b>CVPR</b>)</em>, 2023 </div> <div class="periodical"> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a href="http://arxiv.org/abs/2304.04997" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">arXiv</a> <a href="https://github.com/OreoChocolate/MUREN" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">Code</a> <a href="https://cvlab.postech.ac.kr/research/MUREN/" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">Website</a> </div> <div class="abstract hidden"> <p>Recent state-of-the-art methods for HOI detection typically build on transformer architectures with two decoder branches, one for human-object pair detection and the other for interaction classification. Such disentangled transformers, however, may suffer from insufficient context exchange between the branches and lead to a lack of context information for relational reasoning, which is critical in discovering HOI instances. In this work, we propose the multiplex relation network (MUREN) that performs rich context exchange between three decoder branches using unary, pairwise, and ternary relations of human, object, and interaction tokens. The proposed method learns comprehensive relational contexts for discovering HOI instances, achieving state-of-the-art performance on two standard benchmarks for HOI detection, HICO-DET and V-COCO. </p> </div> </div> </div> </li> <li> <div class="row"> <div class="col col-sm-4 abbr"> <abbr class="badge rounded w-100">CVPR</abbr> <figure> <picture> <img src="/assets/img/publication_preview/SQUAT.PNG" class="preview z-depth-1 rounded" width="100%" height="auto" alt="SQUAT.PNG" data-zoomable loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> <div id="Jung_2023_CVPR" class="col-sm-8"> <div class="title">Devilâ€™s on the Edges: Selective Quad Attention for Scene Graph Generation</div> <div class="author"> Deunsol Jung,Â <em><i>Sanghyun Kim</i> </em>,Â Won Hwa Kim,Â andÂ Minsu Cho </div> <div class="periodical"> <em>In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (<b>CVPR</b>)</em>, 2023 </div> <div class="periodical"> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a href="http://arxiv.org/abs/2304.03495" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">arXiv</a> <a href="https://github.com/hesedjds/SQUAT" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">Code</a> <a href="https://cvlab.postech.ac.kr/research/SQUAT/" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">Website</a> </div> <div class="abstract hidden"> <p>Scene graph generation aims to construct a semantic graph structure from an image such that its nodes and edges respectively represent objects and their relationships. One of the major challenges for the task lies in the presence of distracting objects and relationships in images; contextual reasoning is strongly distracted by irrelevant objects or backgrounds and, more importantly, a vast number of irrelevant candidate relations. To tackle the issue, we propose the Selective Quad Attention Network (SQUAT) that learns to select relevant object pairs and disambiguate them via diverse contextual interactions. SQUAT consists of two main components: edge selection and quad attention. The edge selection module selects relevant object pairs, i.e., edges in the scene graph, which helps contextual reasoning, and the quad attention module then updates the edge features using both edge-to-node and edge-to-edge cross-attentions to capture contextual information between objects and object pairs. Experiments demonstrate the strong performance and robustness of SQUAT, achieving the state of the art on the Visual Genome and Open Images v6 benchmarks</p> </div> </div> </div> </li> </ol> </div> <div class="cv"> <a class="anchor" id="volunteer"></a> <h2 class="card-title font-weight-medium"> Academic Services </h2> <div> <ul class="card-text font-weight-light list-group"> <li class="list-group-item" style="border: none !important;"> <h6 class="" style="font-size: 1.0rem"><b>Reviewer of international conferences</b></h6> <ul class="items" style="list-style-type: circle;"> <li> <span class="item">Computer Vision and Pattern Recognition (CVPR)</span> </li> <li> <span class="item">International Conference on Computer Vision (ICCV)</span> </li> <li> <span class="item">European Conference on Computer Vision (ECCV)</span> </li> <li> <span class="item">Neural Information Processing Systems (NeurIPS)</span> </li> <li> <span class="item">Artificial Intelligence and Statistics Conference (AISTATS)</span> </li> </ul> </li> <li class="list-group-item" style="border: none !important;"> <h6 class="" style="font-size: 1.0rem"><b>Reviewer of international journals</b></h6> <ul class="items" style="list-style-type: circle;"> <li> <span class="item">International Journal of Computer Vision (IJCV)</span> </li> </ul> </li> </ul> </div> </div> </article> </div> </div> <footer class="fixed-bottom" role="contentinfo"> <div class="container mt-0"> Â© Copyright 2025 Sanghyun Kim. Powered by <a href="https://jekyllrb.com/" target="_blank" rel="external nofollow noopener">Jekyll</a> with <a href="https://github.com/alshedivat/al-folio" rel="external nofollow noopener" target="_blank">al-folio</a> theme. Hosted by <a href="https://pages.github.com/" target="_blank" rel="external nofollow noopener">GitHub Pages</a>. Photos from <a href="https://unsplash.com" target="_blank" rel="external nofollow noopener">Unsplash</a>. </div> </footer> <script src="https://cdn.jsdelivr.net/npm/jquery@3.6.0/dist/jquery.min.js" integrity="sha256-/xUj+3OJU5yExlq6GSYGSHk7tPXikynS7ogEvDej/m4=" crossorigin="anonymous"></script> <script src="/assets/js/bootstrap.bundle.min.js"></script> <script src="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/js/mdb.min.js" integrity="sha256-NdbiivsvWt7VYCt6hYNT3h/th9vSTL4EDWeGs5SN3DA=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/masonry-layout@4.2.2/dist/masonry.pkgd.min.js" integrity="sha256-Nn1q/fx0H7SNLZMQ5Hw5JLaTRZp0yILA/FRexe19VdI=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/imagesloaded@5.0.0/imagesloaded.pkgd.min.js" integrity="sha256-htrLFfZJ6v5udOG+3kNLINIKh2gvoKqwEhHYfTTMICc=" crossorigin="anonymous"></script> <script defer src="/assets/js/masonry.js?a0db7e5d5c70cc3252b3138b0c91dcaf" type="text/javascript"></script> <script defer src="https://cdn.jsdelivr.net/npm/medium-zoom@1.1.0/dist/medium-zoom.min.js" integrity="sha256-ZgMyDAIYDYGxbcpJcfUnYwNevG/xi9OHKaR/8GK+jWc=" crossorigin="anonymous"></script> <script defer src="/assets/js/zoom.js?85ddb88934d28b74e78031fd54cf8308"></script> <script src="/assets/js/no_defer.js?2781658a0a2b13ed609542042a859126"></script> <script defer src="/assets/js/common.js?e0514a05c5c95ac1a93a8dfd5249b92e"></script> <script defer src="/assets/js/copy_code.js?12775fdf7f95e901d7119054556e495f" type="text/javascript"></script> <script defer src="/assets/js/jupyter_new_tab.js?d9f17b6adc2311cbabd747f4538bb15f"></script> <script async src="https://d1bxh8uas1mnw7.cloudfront.net/assets/embed.js"></script> <script async src="https://badge.dimensions.ai/badge.js"></script> <script defer type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3.2.2/es5/tex-mml-chtml.js" integrity="sha256-MASABpB4tYktI2Oitl4t+78w/lyA+D7b/s9GEP0JOGI=" crossorigin="anonymous"></script> <script src="/assets/js/mathjax-setup.js?a5bb4e6a542c546dd929b24b8b236dfd"></script> <script defer src="https://cdnjs.cloudflare.com/polyfill/v3/polyfill.min.js?features=es6" crossorigin="anonymous"></script> <script defer src="/assets/js/progress-bar.js?2f30e0e6801ea8f5036fa66e1ab0a71a" type="text/javascript"></script> <script src="/assets/js/vanilla-back-to-top.min.js?f40d453793ff4f64e238e420181a1d17"></script> <script>
    addBackToTop();
  </script> </body> </html>